"""Batch PDF processing orchestrator.

ì±•í„° ê¸°ë°˜ ê³„ì¸µì  PDF ì²˜ë¦¬.
ì±•í„° â†’ ì„¹ì…˜ â†’ ë¬¸ë‹¨ â†’ ì•„ì´ë””ì–´ ì¶”ì¶œ íŒŒì´í”„ë¼ì¸.
"""

import os
from typing import List, Optional
from tqdm import tqdm

from src.pdf.parser import get_pdf_metadata, extract_all_pages
from src.pdf.chunker import split_chapter_into_paragraphs
from src.pdf.chapter_detector import ChapterDetector
from src.pdf.text_normalizer import TextNormalizer
from src.db.connection import get_session
from src.db.models import Book, Chapter
from src.db.operations import (
    create_book,
    get_book_by_title,
    create_chapters_from_detected,
    get_chapters_by_book,
)
from src.db.progress import (
    initialize_chapter_progress,
    get_pending_chapters,
    mark_chapter_processing,
    mark_chapter_completed,
    mark_chapter_failed,
    get_chapter_progress_stats,
    reset_stuck_chapters,
)
from src.model.schemas import ParagraphChunk
from src.workflow.state import State
from src.workflow.nodes import extract_core_idea, save_to_database


def process_pdf(
    pdf_path: str,
    resume: bool = False,
    book_id: Optional[int] = None,
    model_version: str = "gemini-2.5-flash",
) -> dict:
    """
    ì±•í„° ê¸°ë°˜ ê³„ì¸µì  PDF ì²˜ë¦¬.

    íŒŒì´í”„ë¼ì¸:
    1. ì „ì²´ í…ìŠ¤íŠ¸ ì¶”ì¶œ
    2. ì±•í„° ê°ì§€ (TOC/íŒ¨í„´ ê¸°ë°˜)
    3. ì±•í„°ë³„ ë¬¸ë‹¨ ë¶„í• 
    4. ì•„ì´ë””ì–´ ì¶”ì¶œ ë° ì €ì¥

    Args:
        pdf_path: PDF íŒŒì¼ ê²½ë¡œ
        resume: ì¬ê°œ ëª¨ë“œ ì—¬ë¶€
        book_id: ì¬ê°œ ì‹œ ì±… ID
        model_version: LLM ëª¨ë¸ ë²„ì „

    Returns:
        ì²˜ë¦¬ í†µê³„ ë”•ì…”ë„ˆë¦¬
    """
    if not resume and not os.path.exists(pdf_path):
        raise FileNotFoundError(f"PDF íŒŒì¼ ì—†ìŒ: {pdf_path}")

    session = get_session()

    try:
        # Phase 1: ì±… ì„¤ì •
        if resume and book_id:
            book, chapters = _resume_book(session, book_id)
        else:
            book, chapters = _create_book_with_chapters(session, pdf_path)

        # Phase 2: ëŒ€ê¸° ì¤‘ì¸ ì±•í„° í™•ì¸
        pending_chapters = get_pending_chapters(session, book.id)

        if not pending_chapters:
            print("âœ… ëª¨ë“  ì±•í„°ê°€ ì²˜ë¦¬ ì™„ë£Œë¨!")
            return get_chapter_progress_stats(session, book.id)

        print(f"\nğŸš€ {len(pending_chapters)}ê°œ ì±•í„° ì²˜ë¦¬ ì‹œì‘...")

        # Phase 3: ì „ì²´ í…ìŠ¤íŠ¸ ì¶”ì¶œ (1íšŒë§Œ)
        print("ğŸ“„ ì „ì²´ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘...")
        pages = extract_all_pages(pdf_path)
        normalizer = TextNormalizer()
        full_text = normalizer.normalize_full_text(pages)
        print(f"   í…ìŠ¤íŠ¸ ê¸¸ì´: {len(full_text):,} ë¬¸ì")

        # í˜ì´ì§€ë³„ ë¬¸ì ìœ„ì¹˜ ê³„ì‚°
        page_char_positions = _calculate_page_positions(pages)

        # Phase 4: ì±•í„°ë³„ ì²˜ë¦¬
        stats = _process_chapters(
            session=session,
            book=book,
            full_text=full_text,
            page_char_positions=page_char_positions,
            pending_chapters=pending_chapters,
            model_version=model_version,
        )

        # Phase 5: ìš”ì•½ ì¶œë ¥
        _print_summary(stats)

        return stats

    finally:
        session.close()


def _resume_book(session, book_id: int) -> tuple:
    """ì±… ì¬ê°œ."""
    print(f"ğŸ“– ì±… ì¬ê°œ (ID: {book_id})")

    book = session.query(Book).filter_by(id=book_id).first()
    if not book:
        raise ValueError(f"ì±… ID {book_id} ì—†ìŒ")

    chapters = get_chapters_by_book(session, book_id)
    if not chapters:
        raise ValueError(f"ì±… ID {book_id}ì— ì±•í„° ì—†ìŒ")

    # ë©ˆì¶˜ ì±•í„° ë¦¬ì…‹
    stuck_count = reset_stuck_chapters(session, book_id)
    if stuck_count > 0:
        print(f"âš ï¸  {stuck_count}ê°œ ë©ˆì¶˜ ì±•í„° ë¦¬ì…‹")

    return book, chapters


def _create_book_with_chapters(session, pdf_path: str) -> tuple:
    """ì±… ìƒì„± ë° ì±•í„° ê°ì§€."""
    # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
    metadata = get_pdf_metadata(pdf_path)
    print(f"ğŸ“– ì²˜ë¦¬ ì‹œì‘: {metadata['title']}")
    print(f"   ì €ì: {metadata['author']}")
    print(f"   í˜ì´ì§€: {metadata['total_pages']}")

    # ì¤‘ë³µ í™•ì¸
    existing = get_book_by_title(session, metadata['title'])
    if existing:
        print(f"âš ï¸  '{metadata['title']}' ì´ë¯¸ ì¡´ì¬ (ID: {existing.id})")
        print(f"   ì¬ê°œ: --resume --book-id {existing.id}")
        raise ValueError("ì±…ì´ ì´ë¯¸ ì¡´ì¬í•¨")

    # ì±… ìƒì„±
    book = create_book(
        session,
        title=metadata['title'],
        author=metadata['author'],
        source_path=pdf_path,
    )
    print(f"âœ… ì±… ìƒì„± ì™„ë£Œ (ID: {book.id})")

    # ì±•í„° ê°ì§€
    print("ğŸ” ì±•í„° ê°ì§€ ì¤‘...")
    detector = ChapterDetector(pdf_path)
    detected_chapters = detector.detect_chapters()
    print(f"   {len(detected_chapters)}ê°œ ì±•í„° ê°ì§€ë¨")

    for ch in detected_chapters[:5]:  # ì²˜ìŒ 5ê°œë§Œ ì¶œë ¥
        print(f"     - {ch.title} (p.{ch.start_page+1}-{ch.end_page+1})")
    if len(detected_chapters) > 5:
        print(f"     ... ì™¸ {len(detected_chapters)-5}ê°œ")

    # DBì— ì±•í„° ì €ì¥
    chapters = create_chapters_from_detected(session, book.id, detected_chapters)

    # ì§„í–‰ ì¶”ì  ì´ˆê¸°í™”
    initialize_chapter_progress(session, book.id, chapters)

    return book, chapters


def _calculate_page_positions(pages: List[str]) -> List[tuple]:
    """í˜ì´ì§€ë³„ ë¬¸ì ìœ„ì¹˜ ê³„ì‚°."""
    positions = []
    char_offset = 0

    for page_num, page_text in enumerate(pages):
        start = char_offset
        char_offset += len(page_text) + 1  # +1 for join character
        positions.append((page_num, start, char_offset))

    return positions


def _get_chapter_text(
    full_text: str,
    page_positions: List[tuple],
    chapter: Chapter
) -> str:
    """ì±•í„° í…ìŠ¤íŠ¸ ì¶”ì¶œ."""
    # ì‹œì‘/ë í˜ì´ì§€ì˜ ë¬¸ì ìœ„ì¹˜ ì°¾ê¸°
    start_char = 0
    end_char = len(full_text)

    for page_num, start, end in page_positions:
        if page_num == chapter.start_page:
            start_char = start
        if page_num == chapter.end_page:
            end_char = end
            break

    return full_text[start_char:end_char]


def _process_chapters(
    session,
    book: Book,
    full_text: str,
    page_char_positions: List[tuple],
    pending_chapters: List[Chapter],
    model_version: str,
) -> dict:
    """ì±•í„°ë³„ ì²˜ë¦¬ ì‹¤í–‰."""
    stats = {
        'total_chapters': len(pending_chapters),
        'completed': 0,
        'failed': 0,
        'total_paragraphs': 0,
        'total_ideas': 0,
    }

    global_para_idx = 0

    for chapter in tqdm(pending_chapters, desc="ì±•í„° ì²˜ë¦¬"):
        try:
            mark_chapter_processing(session, book.id, chapter.id)

            # ì±•í„° í…ìŠ¤íŠ¸ ì¶”ì¶œ
            chapter_text = _get_chapter_text(
                full_text, page_char_positions, chapter
            )

            if len(chapter_text.strip()) < 100:
                # ë‚´ìš©ì´ ë„ˆë¬´ ì§§ìŒ
                mark_chapter_completed(session, book.id, chapter.id)
                stats['completed'] += 1
                continue

            # ê³„ì¸µì  ë¬¸ë‹¨ ë¶„í• 
            chunks = split_chapter_into_paragraphs(
                chapter_text=chapter_text,
                chapter_id=chapter.id,
                chapter_title=chapter.title,
                base_paragraph_index=global_para_idx,
            )

            stats['total_paragraphs'] += len(chunks)

            # ê° ë¬¸ë‹¨ ì²˜ë¦¬
            for chunk in chunks:
                # ParagraphChunk ìŠ¤í‚¤ë§ˆë¡œ ë³€í™˜
                para_chunk = ParagraphChunk(
                    book_id=book.id,
                    chapter_id=chunk.chapter_id,
                    paragraph_index=chunk.paragraph_index,
                    chapter_paragraph_index=chunk.chapter_paragraph_index,
                    body_text=chunk.text,
                    section_id=chunk.section_id,
                )

                state = State(
                    chunk=para_chunk,
                    book_id=book.id,
                    model_version=model_version,
                )

                # ì•„ì´ë””ì–´ ì¶”ì¶œ
                state = extract_core_idea(state)

                if state.error:
                    continue

                # DB ì €ì¥
                state = save_to_database(state)

                if not state.error:
                    stats['total_ideas'] += 1

            global_para_idx += len(chunks)
            mark_chapter_completed(session, book.id, chapter.id)
            stats['completed'] += 1

        except Exception as e:
            mark_chapter_failed(session, book.id, chapter.id, str(e))
            stats['failed'] += 1
            print(f"\nâŒ ì±•í„° '{chapter.title}' ì‹¤íŒ¨: {e}")

    return stats


def _print_summary(stats: dict) -> None:
    """ì²˜ë¦¬ ìš”ì•½ ì¶œë ¥."""
    print("\n" + "=" * 60)
    print("ì²˜ë¦¬ ìš”ì•½")
    print("=" * 60)
    print(f"ì´ ì±•í„°: {stats['total_chapters']}")
    print(f"ì™„ë£Œ: {stats['completed']}")
    print(f"ì‹¤íŒ¨: {stats['failed']}")
    print(f"ì´ ë¬¸ë‹¨: {stats['total_paragraphs']}")
    print(f"ì¶”ì¶œëœ ì•„ì´ë””ì–´: {stats['total_ideas']}")
    print("=" * 60)


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="PDF ì²˜ë¦¬ (ì±•í„° ê¸°ë°˜)")
    parser.add_argument("--pdf", type=str, required=True, help="PDF íŒŒì¼ ê²½ë¡œ")
    parser.add_argument("--resume", action="store_true", help="ì¤‘ë‹¨ëœ ì²˜ë¦¬ ì¬ê°œ")
    parser.add_argument("--book-id", type=int, help="ì¬ê°œ ì‹œ ì±… ID")
    parser.add_argument("--model", type=str, default="gemini-2.5-flash", help="ëª¨ë¸ ë²„ì „")

    args = parser.parse_args()

    process_pdf(
        pdf_path=args.pdf,
        resume=args.resume,
        book_id=args.book_id,
        model_version=args.model,
    )
