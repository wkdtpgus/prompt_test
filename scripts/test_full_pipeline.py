"""ì „ì²´ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ (DB ì €ì¥ ì—†ì´).

ì±•í„° ê°ì§€ â†’ ë¬¸ë‹¨ ë¶„í•  â†’ ì•„ì´ë””ì–´ ì¶”ì¶œê¹Œì§€ í…ŒìŠ¤íŠ¸.
ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥í•˜ì—¬ ê²€ìˆ˜ ê°€ëŠ¥.

--semantic í”Œë˜ê·¸ë¡œ í•˜ì´ë¸Œë¦¬ë“œ ì²­í‚¹ ë°©ì‹ í…ŒìŠ¤íŠ¸ ê°€ëŠ¥.
"""

import sys
import os
import json
from datetime import datetime

import fitz  # PyMuPDF

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.pdf.parser import extract_full_text, extract_all_pages
from src.pdf.chapter_detector import ChapterDetector
from src.pdf.chunker import split_chapter_into_paragraphs
from src.pdf.text_normalizer import TextNormalizer
from src.model.schemas import ParagraphChunk
from src.workflow.state import State
from src.workflow.nodes import extract_core_idea
from src.pdf.semantic_chunker import (
    hybrid_chunk_and_extract,
    get_semantic_chunking_stats,
)


def extract_chapter_text_from_pdf(pdf_path: str, start_page: int, end_page: int) -> str:
    """
    PDFì—ì„œ í˜ì´ì§€ ë²”ìœ„ë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ í›„ ì •ê·œí™”.

    í˜ì´ì§€ë³„ ë¬¸ì ìœ„ì¹˜ ê³„ì‚°ì˜ ë¶ˆì¼ì¹˜ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´
    ê° ì±•í„°ë§ˆë‹¤ í•´ë‹¹ í˜ì´ì§€ ë²”ìœ„ì˜ í…ìŠ¤íŠ¸ë¥¼ ì§ì ‘ ì¶”ì¶œí•˜ê³  ì •ê·œí™”.

    Args:
        pdf_path: PDF íŒŒì¼ ê²½ë¡œ
        start_page: ì‹œì‘ í˜ì´ì§€ (0-indexed)
        end_page: ë í˜ì´ì§€ (0-indexed, inclusive)

    Returns:
        ì •ê·œí™”ëœ ì±•í„° í…ìŠ¤íŠ¸
    """
    doc = fitz.open(pdf_path)
    try:
        pages = []
        for page_num in range(start_page, min(end_page + 1, len(doc))):
            pages.append(doc[page_num].get_text())

        normalizer = TextNormalizer()
        return normalizer.normalize_full_text(pages)
    finally:
        doc.close()


def test_full_pipeline(
    pdf_path: str,
    max_chapters: int = 3,
    max_paragraphs: int = 5,
    output_path: str = None,
    use_semantic: bool = False
):
    """
    ì „ì²´ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸.

    Args:
        pdf_path: PDF íŒŒì¼ ê²½ë¡œ
        max_chapters: í…ŒìŠ¤íŠ¸í•  ìµœëŒ€ ì±•í„° ìˆ˜
        max_paragraphs: ì±•í„°ë‹¹ ìµœëŒ€ ë¬¸ë‹¨ ìˆ˜
        output_path: ê²°ê³¼ ì €ì¥ ê²½ë¡œ (ê¸°ë³¸: ./output/pipeline_result_YYYYMMDD_HHMMSS.json)
        use_semantic: í•˜ì´ë¸Œë¦¬ë“œ ì˜ë¯¸ì  ì²­í‚¹ ì‚¬ìš© ì—¬ë¶€
    """
    # ê²°ê³¼ ì €ì¥ êµ¬ì¡°
    result = {
        "meta": {
            "pdf_file": os.path.basename(pdf_path),
            "pdf_path": pdf_path,
            "test_time": datetime.now().isoformat(),
            "max_chapters": max_chapters,
            "max_paragraphs_per_chapter": max_paragraphs,
            "chunking_method": "semantic" if use_semantic else "rule_based",
        },
        "extraction": {},
        "chapters": [],
        "stats": {},
    }

    print("=" * 70)
    print(f"ì „ì²´ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸: {os.path.basename(pdf_path)}")
    print(f"ì²­í‚¹ ë°©ì‹: {'í•˜ì´ë¸Œë¦¬ë“œ ì˜ë¯¸ì  ì²­í‚¹' if use_semantic else 'ê·œì¹™ ê¸°ë°˜ ì²­í‚¹'}")
    print("=" * 70)

    # Phase 1: ì „ì²´ í…ìŠ¤íŠ¸ ì¶”ì¶œ
    print("\n[Phase 1] ì „ì²´ í…ìŠ¤íŠ¸ ì¶”ì¶œ...")
    pages = extract_all_pages(pdf_path)
    normalizer = TextNormalizer()
    full_text = normalizer.normalize_full_text(pages)
    print(f"  ì´ í˜ì´ì§€: {len(pages)}")
    print(f"  ì´ ë¬¸ì ìˆ˜: {len(full_text):,}")

    result["extraction"] = {
        "total_pages": len(pages),
        "total_characters": len(full_text),
    }

    # Phase 2: ì±•í„° ê°ì§€
    print("\n[Phase 2] ì±•í„° ê°ì§€...")
    detector = ChapterDetector(pdf_path)
    chapters = detector.detect_chapters()
    print(f"  ê°ì§€ëœ ì±•í„°: {len(chapters)}ê°œ")
    print(f"  ê°ì§€ ë°©ë²•: {chapters[0].detection_method if chapters else 'none'}")

    result["extraction"]["detected_chapters"] = len(chapters)
    result["extraction"]["detection_method"] = chapters[0].detection_method if chapters else 'none'

    # Phase 3: ì±•í„°ë³„ ë¬¸ë‹¨ ë¶„í•  ë° ì•„ì´ë””ì–´ ì¶”ì¶œ
    print(f"\n[Phase 3] ì±•í„°ë³„ ì²˜ë¦¬ (ìµœëŒ€ {max_chapters}ê°œ ì±•í„°)...")

    stats = {
        'chapters_processed': 0,
        'total_paragraphs': 0,
        'total_ideas': 0,
        'failed_extractions': 0,
    }

    # ì‹¤ì œ ë‚´ìš©ì´ ìˆëŠ” ì±•í„°ë§Œ í•„í„°ë§ (Cover, TOC ë“± ì œì™¸)
    content_chapters = [
        ch for ch in chapters
        if ch.level == 1 and 'Chapter' in ch.title
    ]

    if not content_chapters:
        # Chapterê°€ ì—†ìœ¼ë©´ level 1 ì¤‘ í˜ì´ì§€ ë²”ìœ„ê°€ í° ê²ƒë“¤ ì„ íƒ
        content_chapters = [
            ch for ch in chapters
            if ch.level == 1 and (ch.end_page - ch.start_page) > 5
        ]

    if not content_chapters:
        content_chapters = chapters[:max_chapters]

    print(f"  ë‚´ìš© ì±•í„°: {len(content_chapters)}ê°œ ì¤‘ {min(max_chapters, len(content_chapters))}ê°œ ì²˜ë¦¬")

    for chapter in content_chapters[:max_chapters]:
        print(f"\n{'â”€' * 60}")
        print(f"ğŸ“– ì±•í„°: {chapter.title}")
        print(f"   í˜ì´ì§€: {chapter.start_page + 1} - {chapter.end_page + 1}")
        print(f"{'â”€' * 60}")

        # ì±•í„° ê²°ê³¼ êµ¬ì¡°
        chapter_result = {
            "title": chapter.title,
            "start_page": chapter.start_page + 1,
            "end_page": chapter.end_page + 1,
            "level": chapter.level,
            "detection_method": chapter.detection_method,
            "confidence": chapter.confidence,
            "paragraphs": [],
        }

        # ì±•í„° í…ìŠ¤íŠ¸ ì¶”ì¶œ (PDFì—ì„œ ì§ì ‘ í˜ì´ì§€ ë²”ìœ„ë¡œ ì¶”ì¶œ)
        chapter_text = extract_chapter_text_from_pdf(
            pdf_path, chapter.start_page, chapter.end_page
        )
        chapter_result["text_length"] = len(chapter_text)

        if len(chapter_text.strip()) < 100:
            print("  â­ï¸  ë‚´ìš©ì´ ë„ˆë¬´ ì§§ìŒ, ê±´ë„ˆëœ€")
            chapter_result["skipped"] = True
            chapter_result["skip_reason"] = "í…ìŠ¤íŠ¸ 100ì ë¯¸ë§Œ"
            result["chapters"].append(chapter_result)
            continue

        if use_semantic:
            # ===== í•˜ì´ë¸Œë¦¬ë“œ ì˜ë¯¸ì  ì²­í‚¹ =====
            # LLMì´ ë¬¸ë‹¨ ë¶„í•  + ì•„ì´ë””ì–´ ì¶”ì¶œì„ ë™ì‹œì— ìˆ˜í–‰
            chunk_idea_pairs = hybrid_chunk_and_extract(
                chapter_text=chapter_text,
                chapter_id=None,
                chapter_title=chapter.title,
                base_paragraph_index=stats['total_paragraphs'],
            )

            semantic_stats = get_semantic_chunking_stats(chunk_idea_pairs)
            print(f"  ğŸ“ ì˜ë¯¸ì  ë¶„í•  ë¬¸ë‹¨: {semantic_stats['total_paragraphs']}ê°œ")
            print(f"     - ê°œë… ì¶”ì¶œë¨: {semantic_stats['paragraphs_with_concept']}ê°œ")
            print(f"     - ê³ ìœ  ê°œë…: {semantic_stats['unique_concepts']}ê°œ")
            print(f"     - í‰ê·  ë¬¸ë‹¨ ê¸¸ì´: {semantic_stats['avg_paragraph_length']:.0f}ì")

            chapter_result["total_paragraphs"] = semantic_stats['total_paragraphs']
            chapter_result["semantic_stats"] = semantic_stats
            stats['total_paragraphs'] += semantic_stats['total_paragraphs']

            # ë¬¸ë‹¨ë³„ ê²°ê³¼ (ìµœëŒ€ Nê°œ)
            for i, (chunk, concept) in enumerate(chunk_idea_pairs[:max_paragraphs]):
                print(f"\n  â”€â”€ ë¬¸ë‹¨ {i+1}/{min(len(chunk_idea_pairs), max_paragraphs)} â”€â”€")
                print(f"  section_id: {chunk.section_id}, section_title: {chunk.section_title}")
                print(f"  í…ìŠ¤íŠ¸ ({len(chunk.text)}ì):")

                preview = chunk.text[:200].replace('\n', ' ')
                print(f"    \"{preview}...\"")
                print(f"  âœ… ì¶”ì¶œëœ ì•„ì´ë””ì–´: {concept if concept else '(ì—†ìŒ)'}")

                para_result = {
                    "paragraph_index": chunk.paragraph_index,
                    "chapter_paragraph_index": chunk.chapter_paragraph_index,
                    "section_id": chunk.section_id,
                    "section_title": chunk.section_title,
                    "text": chunk.text,
                    "text_length": len(chunk.text),
                    "start_char": chunk.start_char,
                    "end_char": chunk.end_char,
                    "idea_extraction": {
                        "status": "success" if concept else "no_idea",
                        "concept": concept,
                    },
                }

                if concept:
                    stats['total_ideas'] += 1

                chapter_result["paragraphs"].append(para_result)

            if len(chunk_idea_pairs) > max_paragraphs:
                print(f"\n  ... ì™¸ {len(chunk_idea_pairs) - max_paragraphs}ê°œ ë¬¸ë‹¨ ìƒëµ")
                chapter_result["paragraphs_omitted"] = len(chunk_idea_pairs) - max_paragraphs

        else:
            # ===== ê¸°ì¡´ ê·œì¹™ ê¸°ë°˜ ì²­í‚¹ =====
            # ë¬¸ë‹¨ ë¶„í• 
            chunks = split_chapter_into_paragraphs(
                chapter_text=chapter_text,
                chapter_id=None,  # DB ì—†ìœ¼ë¯€ë¡œ None
                chapter_title=chapter.title,
                base_paragraph_index=stats['total_paragraphs'],
            )

            print(f"  ğŸ“ ë¶„í• ëœ ë¬¸ë‹¨: {len(chunks)}ê°œ")
            chapter_result["total_paragraphs"] = len(chunks)
            stats['total_paragraphs'] += len(chunks)

            # ë¬¸ë‹¨ë³„ ì•„ì´ë””ì–´ ì¶”ì¶œ (ìµœëŒ€ Nê°œ)
            for i, chunk in enumerate(chunks[:max_paragraphs]):
                print(f"\n  â”€â”€ ë¬¸ë‹¨ {i+1}/{min(len(chunks), max_paragraphs)} â”€â”€")
                print(f"  section_id: {chunk.section_id}, section_title: {chunk.section_title}")
                print(f"  í…ìŠ¤íŠ¸ ({len(chunk.text)}ì):")

                # í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸° (ì²˜ìŒ 200ì)
                preview = chunk.text[:200].replace('\n', ' ')
                print(f"    \"{preview}...\"")

                # ë¬¸ë‹¨ ê²°ê³¼ êµ¬ì¡°
                para_result = {
                    "paragraph_index": chunk.paragraph_index,
                    "chapter_paragraph_index": chunk.chapter_paragraph_index,
                    "section_id": chunk.section_id,
                    "section_title": chunk.section_title,
                    "text": chunk.text,
                    "text_length": len(chunk.text),
                    "start_char": chunk.start_char,
                    "end_char": chunk.end_char,
                    "idea_extraction": None,
                }

                # ì•„ì´ë””ì–´ ì¶”ì¶œ
                para_chunk = ParagraphChunk(
                    book_id=1,  # ì„ì‹œ
                    chapter_id=None,
                    paragraph_index=chunk.paragraph_index,
                    chapter_paragraph_index=chunk.chapter_paragraph_index,
                    body_text=chunk.text,
                )

                state = State(
                    chunk=para_chunk,
                    book_id=1,
                    model_version="gemini-2.5-flash",
                )

                try:
                    state = extract_core_idea(state)

                    if state.error:
                        print(f"  âŒ ì¶”ì¶œ ì‹¤íŒ¨: {state.error}")
                        stats['failed_extractions'] += 1
                        para_result["idea_extraction"] = {
                            "status": "failed",
                            "error": state.error,
                        }
                    elif state.result:  # State.resultê°€ ExtractedIdeaì„
                        print(f"  âœ… ì¶”ì¶œëœ ì•„ì´ë””ì–´: {state.result.concept}")
                        stats['total_ideas'] += 1
                        para_result["idea_extraction"] = {
                            "status": "success",
                            "concept": state.result.concept,
                        }
                    else:
                        print(f"  âš ï¸  ì•„ì´ë””ì–´ ì—†ìŒ")
                        para_result["idea_extraction"] = {
                            "status": "no_idea",
                        }

                except Exception as e:
                    print(f"  âŒ ì˜¤ë¥˜: {e}")
                    stats['failed_extractions'] += 1
                    para_result["idea_extraction"] = {
                        "status": "error",
                        "error": str(e),
                    }

                chapter_result["paragraphs"].append(para_result)

            if len(chunks) > max_paragraphs:
                print(f"\n  ... ì™¸ {len(chunks) - max_paragraphs}ê°œ ë¬¸ë‹¨ ìƒëµ")
                chapter_result["paragraphs_omitted"] = len(chunks) - max_paragraphs

        result["chapters"].append(chapter_result)
        stats['chapters_processed'] += 1

    # ìš”ì•½
    result["stats"] = stats

    print("\n" + "=" * 70)
    print("í…ŒìŠ¤íŠ¸ ìš”ì•½")
    print("=" * 70)
    print(f"  ì²˜ë¦¬ëœ ì±•í„°: {stats['chapters_processed']}")
    print(f"  ì´ ë¬¸ë‹¨: {stats['total_paragraphs']}")
    print(f"  ì¶”ì¶œëœ ì•„ì´ë””ì–´: {stats['total_ideas']}")
    print(f"  ì¶”ì¶œ ì‹¤íŒ¨: {stats['failed_extractions']}")
    print("=" * 70)

    # ê²°ê³¼ íŒŒì¼ ì €ì¥
    if output_path is None:
        os.makedirs("output", exist_ok=True)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_path = f"output/pipeline_result_{timestamp}.json"

    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2)

    print(f"\nğŸ“ ê²°ê³¼ ì €ì¥: {output_path}")

    return result


def main():
    import argparse

    parser = argparse.ArgumentParser(
        description="PDF íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ (ì±•í„° ê°ì§€ â†’ ë¬¸ë‹¨ ë¶„í•  â†’ ì•„ì´ë””ì–´ ì¶”ì¶œ)",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì˜ˆì‹œ:
  python test_full_pipeline.py ./AI_Engineering.pdf
  python test_full_pipeline.py ./AI_Engineering.pdf --chapters 2 --paragraphs 3
  python test_full_pipeline.py ./AI_Engineering.pdf --semantic
  python test_full_pipeline.py ./AI_Engineering.pdf --semantic --chapters 1 --paragraphs 5
        """
    )

    parser.add_argument("pdf_path", help="PDF íŒŒì¼ ê²½ë¡œ")
    parser.add_argument("--chapters", "-c", type=int, default=3, help="í…ŒìŠ¤íŠ¸í•  ìµœëŒ€ ì±•í„° ìˆ˜ (ê¸°ë³¸: 3)")
    parser.add_argument("--paragraphs", "-p", type=int, default=5, help="ì±•í„°ë‹¹ ìµœëŒ€ ë¬¸ë‹¨ ìˆ˜ (ê¸°ë³¸: 5)")
    parser.add_argument("--output", "-o", help="ê²°ê³¼ ì €ì¥ ê²½ë¡œ")
    parser.add_argument("--semantic", "-s", action="store_true",
                        help="í•˜ì´ë¸Œë¦¬ë“œ ì˜ë¯¸ì  ì²­í‚¹ ì‚¬ìš© (LLMì´ ë¬¸ë‹¨ ë¶„í•  + ì•„ì´ë””ì–´ ì¶”ì¶œ ë™ì‹œ ìˆ˜í–‰)")

    args = parser.parse_args()

    if not os.path.exists(args.pdf_path):
        print(f"ì˜¤ë¥˜: íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.pdf_path}")
        sys.exit(1)

    test_full_pipeline(
        pdf_path=args.pdf_path,
        max_chapters=args.chapters,
        max_paragraphs=args.paragraphs,
        output_path=args.output,
        use_semantic=args.semantic,
    )


if __name__ == "__main__":
    main()
